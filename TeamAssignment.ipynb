{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive analytics\n",
    "\n",
    "# Temporal Demand Patterns & Seasonality # Lina\n",
    "- Pro Tag / Pro Woche / Pro Monat / Pro Jahr \n",
    "- Mit Wetterdaten vergleichen\n",
    " \n",
    "\n",
    "## Geographical Demand Patterns # Nico, Jieyu\n",
    "## ? Pro Stadtteil angemeldete Autos\n",
    "## Altersgruppen je Stadtteil -> Zusammenhang zu Abo Modellen\n",
    "## Pass -> Häufig für den Arbeitsweg? (RoundTrip, OneWay, Zeiten)\n",
    "\n",
    "## KPIs\n",
    "- Aktuelle Auslastung (Live & Historisch, Pro Station) (-> Mehr Stationen, Fahrräder?) # Lukas \n",
    "- Ausfall/Probleme: (Wartungsaufwand (z.B. in Prozent available/not available), ) # Ange\n",
    "- Umsatz durch \"Minutengeld\" # Marc\n",
    "- Anteil an Arten von Passhaltern?\n",
    "- Anteil von Tripcategories?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pp\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import glob\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing & cleaning data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob.glob('Datasets/RideIndego 2016/*.csv')\n",
    "df = pd.concat([pd.read_csv(f) for f  in csv_files], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any Row with NaN 0 in end_lat end_lon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['start_lat'] = pd.to_numeric(df['start_lat'], errors='coerce')\n",
    "df['start_lon'] = pd.to_numeric(df['start_lon'], errors='coerce')\n",
    "df['start_station_id'] = pd.to_numeric(df['start_station_id'], errors='coerce')\n",
    "df['end_station_id'] = pd.to_numeric(df['end_station_id'], errors='coerce')\n",
    "df['end_lat'] = pd.to_numeric(df['end_lat'], errors='coerce')\n",
    "df['end_lon'] = pd.to_numeric(df['end_lon'], errors='coerce')\n",
    "df['bike_id'] = pd.to_numeric(df['bike_id'], errors='coerce')\n",
    "df['plan_duration'] = pd.to_numeric(df['plan_duration'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df.dropna(how='any').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Inspectation of data to prepare descriptive analysis (Marc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alteration of start_time and end_time to datetime type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['start_time'] = pd.to_datetime(df['start_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['end_time'] = pd.to_datetime(df['end_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the length of a trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"duration_trip\"] = df[\"end_time\"] - df[\"start_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am breaking the datetime of start_time into smaller parts(days, time, hour) to make it possible to visualize the usage over different periods. I am taking the start_time, not the end_time since it shows the demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['start_time'].dt.strftime('%m-%d')\n",
    "#here something is not working correctly yet, has to be further examined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time\"] = df[\"start_time\"].apply(lambda dt: dt.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hour\"] = df[\"start_time\"].apply(lambda dt: dt.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"weekday\"] = df[\"start_time\"].apply(lambda dt: dt.dayofweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month\"] = df[\"start_time\"].apply(lambda dt: dt.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"full_date\"] = [d.date() for d in df[\"start_time\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_hour'] = df['start_time'].dt.strftime('%m-%d-%H')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a later purpose we will convert the duration from seconds to minutes, dividing it by 60:\n",
    "df[\"duration\"] = df[\"duration\"] / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection of new dataframe, uncomment if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.date_hour.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including the weather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import weather data\n",
    "#STR_Nov = pd.read_csv(\"Car2Go_STR_SampleData.csv\", encoding = \"ISO-8859-1\")\n",
    "weather = pd.read_csv('weather_hourly_philadelphia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['date_time'] = pd.to_datetime(weather['date_time'])\n",
    "weather['date'] = weather['date_time'].dt.strftime('%m-%d')\n",
    "weather['hour'] = weather[\"date_time\"].apply(lambda dt: dt.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['date_hour'] = weather['date_time'].dt.strftime('%m-%d-%H')\n",
    "weather['year'] = pd.DatetimeIndex(weather['date_time']).year\n",
    "weather = weather[weather['year'] == 2016]\n",
    "\n",
    "#pd. DatetimeIndex(df['date']). year.\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine both dataframes into one using mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to use something else than a full merge as it duplicates everything from df\n",
    "df_weather = pd.merge(df, weather, on=\"date_hour\", how=\"left\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us take another look at the merged dataframe:\n",
    "df_weather.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Descriptive analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Temporal Demand Patterns and Seasonality (Lina, Marc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the demand per hour\n",
    "#hourly_demand = df.groupby([\"hour\"]).agg(demand=(\"bike_id\", 'count'))\n",
    "#hourly_demand = pd.DataFrame(hourly_demand)\n",
    "#hourly_demand\n",
    "hourly_demand = df.groupby([\"date\",\"hour\"])[\"trip_id\"].nunique()\n",
    "hourly_demand = pd.DataFrame(hourly_demand)\n",
    "hourly_demand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pp.subplots(figsize=(10,4))\n",
    "\n",
    "sns.barplot(x=hourly_demand.index.get_level_values(1), y=hourly_demand[\"trip_id\"],ax=ax)\n",
    "pp.xlabel('Hour', fontdict={'size':11})\n",
    "pp.ylabel(\"Demand\", fontdict={\"size\":11})\n",
    "pp.title(\"Overview of hourly demand\", fontsize=14.0, fontweight='bold')\n",
    "\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demand per weekday:\n",
    "weekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "#daily_demand = df.groupby([\"weekday\"]).agg(demand=(\"bike_id\", 'count'))\n",
    "daily_demand = df.groupby([\"month\",\"weekday\"])[\"trip_id\"].nunique()\n",
    "daily_demand = pd.DataFrame(daily_demand)\n",
    "daily_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_demand.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pp.subplots(figsize=(10,4))\n",
    "\n",
    "sns.boxplot(x=daily_demand.index.get_level_values(1),y=daily_demand[\"trip_id\"],palette = 'Blues_d')\n",
    "\n",
    "pp.ylabel(\"Demand\", fontsize=11)\n",
    "pp.title(\"Use of bicycles in one week\", fontsize=14, fontweight=\"bold\", color=\"k\")\n",
    "pp.tight_layout()\n",
    "pp.gca().set_xticklabels(\\\n",
    "    ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']);\n",
    "\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the monthly demand:\n",
    "# different approach:\n",
    "#monthly_demand = df.groupby([\"month\"]).agg(demand=(\"bike_id\", 'count'))\n",
    "monthly_demand = df.groupby([\"month\"])[\"trip_id\"].nunique()\n",
    "\n",
    "monthly_demand = pd.DataFrame(monthly_demand)\n",
    "monthly_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = pp.subplots(figsize=(10,4))\n",
    "\n",
    "sns.barplot(x=monthly_demand.index.get_level_values(0), y=monthly_demand[\"trip_id\"],ax=ax)\n",
    "pp.xlabel(\"Month\", fontsize=11)\n",
    "pp.ylabel(\"Demand\", fontsize=11)\n",
    "sns.set_color_codes(\"dark\")\n",
    "pp.title(\"The monthly use of bicycles \", fontsize=16, fontweight=\"bold\", color=\"m\")\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) KPIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first KPI is the current gross revenue which is calculated by the duration of the trip times $0,15 which is listed as the price/minute on Indego's website. We have to make some adjustments based depending on the type of pass the user has. For that we create a new dataframe only containing relevant data for us which includes the duration of the trip, the ID, the passtype, as well as some time info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_per_minute = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = df[['trip_id', 'duration', 'start_time', 'end_time', 'passholder_type', 'hour','date_hour']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we take the price per minute and the duration of the trip is in seconds, we are going to divide the duration by 60 to get it in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rev[\"duration\"] = rev[\"duration\"]/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rev['passholder_type'].unique()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev['relevant_duration'] = rev.duration + rev.passholder_type.map( lambda x: -30 if x == 'Walk-up' else -60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev.loc[rev.relevant_duration <= 0, \"relevant_duration\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev[\"rev_flex\"] = rev[\"relevant_duration\"] * price_per_minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rev[\"rev_flex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_rev = rev.groupby([\"date_hour\"]).agg(revenue=(\"rev_flex\", 'sum'))\n",
    "hourly_rev = pd.DataFrame(hourly_rev)\n",
    "hourly_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pp.subplots(figsize=(10,4))\n",
    "\n",
    "sns.lineplot(data=hourly_rev, palette=\"tab10\", linewidth=2.5)\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another KPI could be the share of walkups vs. passcard holder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_shares = df[['passholder_type', 'date_hour']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_shares['type'] = type_shares.passholder_type.map( lambda x: 0 if x == 'Walk-up' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_shares.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_share = type_shares.groupby([\"date_hour\", \"type\"]).count()\n",
    "hourly_share = pd.DataFrame(hourly_share)\n",
    "hourly_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_share_per = hourly_share.groupby(level=0).apply(lambda x:\n",
    "                                                 100 * x / float(x.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_share_per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Task 3: Predictive Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Philadelphia weather in 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the weather over the year:\n",
    "#different approach:\n",
    "weather_month = df_weather.groupby([\"month\"]).agg(Temperature =(\"max_temp\", 'mean'))\n",
    "weather_month = pd.DataFrame(weather_month)\n",
    "weather_month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pp.subplots(figsize=(12,6))\n",
    "\n",
    "sns.lineplot(x=weather_month.index.get_level_values(0), y=weather_month[\"Temperature\"],ax=ax)\n",
    "dim=np.arange(1,13,1)\n",
    "pp.xticks(dim)\n",
    "pp.xlabel(\"Months\", fontsize=11)\n",
    "pp.ylabel(\"Temperature\", fontsize=11)\n",
    "pp.title(\"2016: Weather in Philadelphia\", fontsize=14, fontweight=\"bold\", color=\"b\")\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demand and temperatures in comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pp.subplots(figsize=(12,6))\n",
    "\n",
    "sns.barplot(x=monthly_demand.index.get_level_values(0), y=monthly_demand[\"trip_id\"],ax=ax)\n",
    "sns.set_color_codes(\"pastel\")\n",
    "ax2=ax.twinx()\n",
    "sns.lineplot(x=weather_month.index.get_level_values(0), y=weather_month[\"Temperature\"],ax=ax2, color=\"r\", linewidth=2.5)\n",
    "dim=np.arange(0,13,1)\n",
    "pp.xticks(dim)\n",
    "\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future demand is a key factor that will steer operational decision making of a shared rental network. As a data scientist it is your responsibility to facilitate this type of decision support. For the purpose of this assignment we will be interested in forecasting total system-level demand in the next hour. To do so, develop a prediction model that predicts bike rental demand as a function of suitable features available in or derived from the datasets (incl. the weather data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, i am going to create a table that contains the demand of the hour along with some features as temperature, the hour of course, the day of the week and the time of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = df[[\"trip_id\",\"weekday\", \"month\"]].groupby([\"weekday\"])'\n",
    "\n",
    "new_df_merged = pd.merge(weather, hourly_demand, on=[\"date\", \"hour\"], how = \"left\")\n",
    "new_df_merged[\"demand\"] = new_df_merged[\"trip_id\"]\n",
    "new_df_merged[\"weekday\"] = new_df_merged[\"date_time\"].apply(lambda dt: dt.dayofweek)\n",
    "new_df_merged[\"month\"] = new_df_merged[\"date_time\"].apply(lambda dt: dt.month)\n",
    "new_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns that are not relevant for the regression\n",
    "new_df_merged.drop(columns = ['trip_id', 'year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i check whether holidays like the 4th of July are having an impact on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "july = new_df_merged[new_df_merged['month'] == 7]\n",
    "july"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "july_grouped = july.groupby([\"date\"])[\"demand\"].sum()\n",
    "july_grouped = pd.DataFrame(july_grouped)\n",
    "july_grouped\n",
    "#monthly_demand = df.groupby([\"month\"])[\"trip_id\"].nunique()\n",
    "\n",
    "#monthly_demand = pd.DataFrame(monthly_demand)\n",
    "#monthly_demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = pp.subplots(figsize=(20,4))\n",
    "\n",
    "sns.lineplot(x=july_grouped.index.get_level_values(0), y=july_grouped[\"demand\"],ax=ax)\n",
    "pp.xlabel('Day', fontdict={'size':11})\n",
    "pp.ylabel(\"Demand\", fontdict={\"size\":11})\n",
    "pp.title(\"Overview of hourly demand\", fontsize=14.0, fontweight='bold')\n",
    "\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = new_df_merged\n",
    "#df_1[\"summer\"] = df_1[(df_1[\"month\"] > 5) & (df_1[\"month\"] < 12)]\n",
    "#df_1[\"IsWeekday\"] = df_1[\"weekday\"].apply(lambda x: 1 if x<=4 else 0)\n",
    "df_1[\"IsWeekday\"] = df_1[\"weekday\"].apply(lambda x: 1 if x<=4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[\"summer\"] = df_1[\"month\"].apply(lambda x: 1 if (x> 5 & x<12) else 0)\n",
    "df_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_1[[\"max_temp\", \"IsWeekday\", \"hour\", \"precip\", \"summer\"]]\n",
    "y = df_1[[\"demand\"]]\n",
    "y['demand'] = df_1['demand'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data to avoid overfitting and enable testing\n",
    "features=list(zip(X[\"max_temp\"],X[\"precip\"], X[\"IsWeekday\"], X[\"summer\"], X[\"hour\"]))\n",
    "# Do a 70-30 split first\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.3,random_state=34 )\n",
    "\n",
    "# now split X_train to achive 50-20-30 split\n",
    "X_train, X_hold, y_train, y_hold = train_test_split(X_train, y_train, test_size=(0.2/0.7),random_state=34 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression(fit_intercept=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.fit(X_train, y_train)\n",
    "print(linear_model.coef_, linear_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_L1.fit(X_train, y_train)\n",
    "predict = model_L1.predict(X_test)\n",
    "r2_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decession Tree Algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the optimal tree depth\n",
    "find_tree_depth (features,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement the algorithm\n",
    "Tree_reg = DecisionTreeRegressor(max_depth=9)\n",
    "tree_model = Tree_reg.fit(X_train, y_train) \n",
    "\n",
    "#Predict\n",
    "y_hat_tree = tree_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2:\",r2_score(y_test, y_hat_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE:\n",
    "\n",
    "def find_tree_depth (x,y):\n",
    "    \n",
    "    # define list for collecting results\n",
    "    err_train = [] \n",
    "    err_test = []\n",
    "    \n",
    "    # split data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=10)\n",
    "    \n",
    "    #loop over max_depth\n",
    "    \n",
    "    for n in np.arange(1,21): # lets test until 24 for now\n",
    "        \n",
    "        # fit model\n",
    "        \n",
    "        tree_reg = DecisionTreeRegressor(max_depth=n)\n",
    "        tree_model = tree_reg.fit(x_train,y_train)\n",
    "        \n",
    "        # compute errors\n",
    "        \n",
    "        err_train.append(mean_absolute_error(y_train, tree_model.predict(x_train)))\n",
    "        err_test.append(mean_absolute_error(y_test, tree_model.predict(x_test)))\n",
    "\n",
    "\n",
    "    pp.figure(figsize = (8,6))\n",
    "    pp.plot(np.arange(1,21), err_train,np.arange(1,21), err_test)\n",
    "    pp.legend([\"Training\", \"Validation\"])\n",
    "    pp.xlabel(\"Max Tree Depth\")\n",
    "    pp.ylabel(\"MAE\")\n",
    "    pp.title(\"Search over max_depth parameter\",fontsize=14)\n",
    "    #plt.ylim((0,1))\n",
    "    pp.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are taking a look at the KNN Algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_reg = KNeighborsRegressor(n_neighbors=11)\n",
    "KNN_model = KNN_reg.fit(X_train, y_train) \n",
    "\n",
    "# Predict\n",
    "y_hat_KNN = KNN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set performance:\")\n",
    "\n",
    "print(\"MAE:\",mean_absolute_error(y_hat_KNN, y_test), \"demand\")\n",
    "print(\"RMSE:\",(mean_squared_error(y_hat_KNN, y_test))**(0.5), \"demand\")  \n",
    "print(\"R2:\",r2_score(y_test, y_hat_KNN))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
